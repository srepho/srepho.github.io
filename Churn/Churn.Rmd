Churn Analysis
========================================================


### What We Are Trying To Do
One of the more common tasks in Business Analytics is to try and understand consumer behaviour. By understanding the hope is that a company can better change this behaviour. In many industries it is more expensive to find a new customer than to entice an existing one to stay. This is usually known as "churn" analysis. The aim is to accurately identify the cohort who is likely to leave early enough so that the relationship can be saved.

### Data Set

This data is taken from a telecommunications company and involves customer data for a collection of customers who either stayed with the company or left within a certain period. <i>In many industries it's often not the case that the cut off is so binary. Frequently it might be more likely that a client account lays dormant rather than getting explicitly closed - for example if the client only pays for usage. I will explain how to adjust for these situations later in the piece.</i> This dataset is taken from [here](http://www.dataminingconsultant.com/data/churn.txt) with descriptions of the data available [here](http://www.sgi.com/tech/mlc/db/churn.names). This dataset also appears in the [C50 package](http://cran.r-project.org/web/packages/C50/index.html)


Let's load the required packages:

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', message=FALSE}
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(reshape2)
library(caret)
```


Next up we need to load and split the data. The purpose of the split is to try and have a more accurate estimate of how our algorithm/s will go in production. If we were to assess the accuracy on the same data that we use to tune the algorithm, we will have an unrealistically optimistic sense of the model. So we cut up the data and keep a "test" set which we never use to build the model. We can then run the model on "unseen" data and get a more accurate idea of the model's worth.


We can use the Caret package to split the data into a "Training Set" with 75% of the data and a "Test" Set with 25%.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
#churn <- read.csv("D:/Users/soates/Downloads/churn.txt", header=T)
churn <- read.csv("E:/Github Stuff/srepho.github.io/Churn/churn.txt", header=T)
churn$Churned<-churn$Churn.
churn$Churn.<-NULL
set.seed(12)
trainIndex <- caret::createDataPartition(churn$Churned, p = .75, list = FALSE, times = 1)
churnTrain <- churn[ trainIndex,]
churnTest <- churn[-trainIndex, ]
```

So the first step is to have a look at the balance of the outcomes. In this case it's very binary, either the client has an existing contract with our telecommunications company or they have cancelled it.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
table(churnTrain$Churned)
```

So we can see from this that about ~15% of customers left our service. So we now have a baseline for naive prediction of ~85% if we just predict all customers will stay with us (which would not be very useful).

The next step is to have a look at the data - let's start by just looking at numerical summaries. The main things we are looking for are:

1. Missing Data: In particular is the missing data randomly spread or is it affecting a particular cohort? Can we impute the data or should we just ignore it?

2. Errors: These are often of two kinds. Sometimes just obvious errors that stand out (someone might be listed as an impossible age like in our case less than say 16 and more then 85 might be suspicious) or sometimes when the initial data was entered a "special" number is used to indicate it's a missing field. Often this is something like 999999 or -99999 etc and it really stands out from the other values.

3. Very low or high variance: Indicators that are nearly 100% one value are not likely to be very useful and in fact in many algorithms can have a detrimental affect. (If in doubt we can always run the algorithm twice and see which one does better). Conversely if an indicator is a randomly assigned label (think of an ID number) it is providing no information and can be discarded.




```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
summary(churnTrain)
```

1. We can see that there are no missing data in this set (something that almost never happens in real life!). 

2. I cannot see any super obvious mistakes though some of the fields seem a little uncertain. In particular:

  * Its not clear what length of time these fields cover? I am guessing it's an average of something (because otherwise the longer held accounts would have higher fields) but is it a daily, weekly or monthly average? It's hard to tell if these are unrealistic values without knowing the time covered. (As long as the data is correct though it won't matter what the ratio is as long as it's standardised)
  * We need to have a closer look at the States and Phone Fields as we cannot tell if there are errors from this summary.
  
```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
table(churnTrain$State)
```

We can also see some features that are being treated as numerical that should not be (for example Area.Code is treated as a numerical factor when it should be a categorical factor as the numbers are not related to each other). The other factor that stands out is that the Phone field is not really usable as it currently stands as it seems that each phone number is singular. We will drop this variable for the moment. (Depending on the time frame we had for the project or if it was a competition we would spend time looking at the structure of the phone number to try and extract something more meaningful).

So let's switch the Area Code to a categorical factor and drop the Phone field.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
churnTrain$Phone<-NULL
churnTest$Phone<-NULL
churnTrain$Area.Code<-as.factor(churnTrain$Area.Code)
churnTest$Area.Code<-as.factor(churnTest$Area.Code)
```

The next step is to have a close look at the variables graphically. Often looking graphically is more illuminating than just a summary (and famously an identical numerical summary can mask vast differences). We also want to see the shape of the distributions. Some algorithms work more accurately if we can transform highly skewed data to more closely resemble a standard (Gaussian) distribution. Sometimes it's as simple as a log transformation, sometimes we need something like a Box-Cox transformation. It is usually good practice to scale and centre our data as well as the difference in magnitude will trip up some algorithms. We won't have to worry about this today as the Random Forest algorithim does not require these changes.

We can also start to form testable ideas about relationships. For example does the "Account Length" field have an impact on if they churn?


```{r fig.width=5, fig.height=5, cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', fig.show='hold'}
one<-ggplot(churn, aes(x=Account.Length, fill=Churned))+geom_density()+ facet_grid(Churned ~ .) + labs(title="Account Length")
one 
```

Nope! Does not appear to be any noticable difference.

I actually looked at all the variables but I won't bore you with all of them ;)

Next up we can start the process of building an actual model. We will use the excellent `caret` package for this. As mentioned the imbalance in the number of churned customers and the fact that we really want to predict who will be a churned customer mean we have to make some modifications to our approach. More technically we are interested in sensitivity in our models rather then specificity.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', fig.show='hold'}
set.seed(12)
rfmodel<-train(churnTrain$Churned~., data=churnTrain, method="rf", trainControl = c(method = "adaptive_cv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, adaptive = list(min = 10, alpha = 0.05, method = "gls", complete = TRUE)), metric="Kappa")
confusionMatrix(rfmodel)
```

Now we can't be too excited yet as this is how we did on the same data we used to build the model. So let's have a look how it goes with the "test" set that our model has never seen before.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', fig.show='hold'}
pred<-predict(rfmodel, newdata=churnTest)
confusionMatrix(pred, churnTest$Churned)
```

Not too shabby. Of the 97 that we predicted would have left we got 85 right and only 12 wrong. We did miss 35 other clients who left but it's a big improvement from a naive guess of "All Stay" that we started with. In a later post I will show how we can try and capture more of those missing 35 by accepting a few more "false positives".

##### References
* Chapter 16 of the excellent Applied Predictive Modeling by Max Kuhn & Kjell Johnson covers cases like this where there is a class imbalance.
* A new feature of the `caret` package (authored by Max Kuhn) is adaptive resampling. The package website contains an [overview](http://topepo.github.io/caret/adaptive-resampling.html).


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52218028-1', 'srepho.github.io');
  ga('send', 'pageview');

</script>

